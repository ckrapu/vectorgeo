{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 files in S3\n",
      "Found 40 files to run\n",
      "...Downloading vectors/vector-upload-1694210549.parquet from S3\n",
      "File /home/ubuntu/vectorgeo/tmp/vector-upload-1694210549.parquet already exists; skipping download\n",
      "...Uploading vectors/vector-upload-1694210549.parquet to Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Downloading vectors/vector-upload-1694210589.parquet from S3\n",
      "File /home/ubuntu/vectorgeo/tmp/vector-upload-1694210589.parquet already exists; skipping download\n",
      "...Uploading vectors/vector-upload-1694210589.parquet to Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Downloading vectors/vector-upload-1694210632.parquet from S3\n",
      "File /home/ubuntu/vectorgeo/tmp/vector-upload-1694210632.parquet already exists; skipping download\n",
      "...Uploading vectors/vector-upload-1694210632.parquet to Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:02<00:19,  4.50it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vectorgeo.transfer import download_file\n",
    "from vectorgeo import constants as c\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "CHECK_DELAY = 60\n",
    "\n",
    "# Load secrets (adjust the path as necessary)\n",
    "secrets = yaml.load(open('secrets.yml'), Loader=yaml.FullLoader)\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=secrets['aws_access_key_id'], aws_secret_access_key=secrets['aws_secret_access_key'])\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(\n",
    "    url=secrets['qdrant_url'], \n",
    "    api_key=secrets['qdrant_api_key']\n",
    ")\n",
    "\n",
    "# Specify your bucket name and prefix\n",
    "bucket_name = c.S3_BUCKET\n",
    "prefix = 'vectors/'\n",
    "\n",
    "# Create set of files already run\n",
    "checked_keys = set()\n",
    "\n",
    "while True:\n",
    "\n",
    "    # List all Parquet files in the S3 bucket with the specified prefix\n",
    "\n",
    "    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    print(f\"Found {len(objects['Contents'])} files in S3\")\n",
    "\n",
    "    # Filter out the files that have already been run\n",
    "    objects['Contents'] = [\n",
    "        obj for obj in objects['Contents']\n",
    "        if obj['Key'] not in checked_keys and '.parquet' in obj['Key']\n",
    "    ]\n",
    "    print(f\"Found {len(objects['Contents'])} files to run\")\n",
    "\n",
    "    for obj in objects['Contents']:\n",
    "        print(f\"...Downloading {obj['Key']} from S3\")\n",
    "        basename = os.path.basename(obj['Key'])\n",
    "        local_path = os.path.join(c.TMP_DIR, basename)\n",
    "        download_file(obj['Key'], local_path)\n",
    "        \n",
    "        # Load the data into a Pandas DataFrame\n",
    "        df = pd.read_parquet(local_path)\n",
    "        \n",
    "        # Extract vectors and other necessary information\n",
    "        print(f\"...Uploading {obj['Key']} to Qdrant\")\n",
    "        \n",
    "        for df_piece in tqdm(np.array_split(df, 100)):\n",
    "        \n",
    "            points = [\n",
    "                PointStruct(\n",
    "                    id=row['id'],\n",
    "                    vector=row['vector'],\n",
    "                    payload={\"location\": {\"lon\": row['lng'], \"lat\": row['lat']}}\n",
    "                )\n",
    "                for _, row in df_piece.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Batch the vectors and upload them to Qdrant\n",
    "            qdrant_client.upsert(\n",
    "                collection_name=c.QDRANT_COLLECTION_NAME,\n",
    "                wait=True,\n",
    "                points=points\n",
    "            )\n",
    "\n",
    "        # Add the file to the set of files that have already been run\n",
    "        checked_keys.add(obj['Key'])\n",
    "    time.sleep(CHECK_DELAY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectorgeo",
   "language": "python",
   "name": "vg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
