{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/madeline/Dropbox/projects/vectorgeo/tmp/world.gpkg already exists; skipping download\n",
      "File /Users/madeline/Dropbox/projects/vectorgeo/tmp/resnet-triplet-lc.pt already exists; skipping download\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m local_model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39mTMP_DIR, model_filename)\n\u001b[1;32m     56\u001b[0m transfer\u001b[39m.\u001b[39mdownload_file(key, local_model_path)\n\u001b[0;32m---> 57\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(local_model_path)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     59\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded model from \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import vectorgeo.constants as c\n",
    "import vectorgeo.transfer as transfer\n",
    "from vectorgeo.h3_utils import H3GlobalIterator\n",
    "from vectorgeo.landcover import LandCoverPatches\n",
    "\n",
    "# Parameters\n",
    "wipe_qdrant = False\n",
    "inference_batch_size = 32\n",
    "h3_resolution = 7\n",
    "image_size = 32\n",
    "model_filename = \"resnet-triplet-lc.pt\"\n",
    "embed_dim = 16\n",
    "seed_latlng = (47.475099, -122.170557)  # Seattle, WA\n",
    "max_iters = None\n",
    "qdrant_collection = c.QDRANT_COLLECTION_NAME\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "# Load secrets\n",
    "secrets = yaml.load(open(os.path.join(c.BASE_DIR, 'secrets.yml')), Loader=yaml.FullLoader)\n",
    "\n",
    "# Download world geometry\n",
    "world_path = os.path.join(c.TMP_DIR, 'world.gpkg')\n",
    "transfer.download_file('misc/world.gpkg', world_path)\n",
    "world_gdf = gpd.read_file(world_path)\n",
    "world_geom = world_gdf.iloc[0].geometry.simplify(0.1)\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "        url=secrets['qdrant_url'],\n",
    "        api_key=secrets['qdrant_api_key']\n",
    "    )\n",
    "\n",
    "# Wipe Qdrant collection if needed\n",
    "if wipe_qdrant:\n",
    "    print(f\"Wiping Qdrant collection {qdrant_collection}\")\n",
    "    \n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=qdrant_collection,\n",
    "        vectors_config=VectorParams(size=embed_dim, distance=Distance.DOT),\n",
    "    )\n",
    "\n",
    "# Load the PyTorch model\n",
    "key = f\"models/{model_filename}\"\n",
    "local_model_path = os.path.join(c.TMP_DIR, model_filename)\n",
    "transfer.download_file(key, local_model_path)\n",
    "model = torch.load(local_model_path).to(device)\n",
    "model.eval()\n",
    "print(f\"Loaded model from {key}\")\n",
    "\n",
    "# Download land cover data\n",
    "lc_key = 'raw/' + c.COPERNICUS_LC_KEY\n",
    "transfer.download_file(lc_key, c.LC_LOCAL_PATH)\n",
    "lcp = LandCoverPatches(c.LC_LOCAL_PATH, world_gdf, image_size, full_load=False)\n",
    "\n",
    "# Initialize H3 iterator\n",
    "state_filepath = os.path.join(c.TMP_DIR, c.H3_STATE_FILENAME)\n",
    "try:\n",
    "    transfer.download_file('misc/h3-state.json', state_filepath)\n",
    "except Exception as e:\n",
    "    print(f\"Encountered exception {e} while downloading state file\")\n",
    "    print(\"No state file found; starting from scratch\")\n",
    "\n",
    "iterator = H3GlobalIterator(seed_latlng[0], seed_latlng[1], h3_resolution, state_file=state_filepath)\n",
    "int_map       = {x: i for i, x in enumerate(c.LC_LEGEND.keys())}\n",
    "int_map_fn    = np.vectorize(int_map.get)\n",
    "\n",
    "# Main inference loop\n",
    "h3_batch = []\n",
    "xs_batch = []\n",
    "h3s_processed = set()\n",
    "\n",
    "for i, cell in enumerate(tqdm(iterator)):\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"Processing cell {i}: {cell}\")\n",
    "        iterator.save_state()\n",
    "        transfer.upload(c.H3_STATE_KEY, state_filepath)        \n",
    "\n",
    "    if max_iters and i >= int(max_iters):\n",
    "        print(f\"Reached max_iters {max_iters}; stopping\")\n",
    "        break\n",
    "\n",
    "    poly = Polygon((x, y) for y, x in h3.h3_to_geo_boundary(cell))\n",
    "\n",
    "    if not world_geom.intersects(poly):\n",
    "        h3s_processed.add(cell)\n",
    "        continue\n",
    "\n",
    "    xs = int_map_fn(lcp.h3_to_patch(cell))\n",
    "\n",
    "    xs_one_hot = np.zeros((c.LC_N_CLASSES, image_size, image_size))\n",
    "\n",
    "    for i in range(c.LC_N_CLASSES):\n",
    "        xs_one_hot[i] = (xs == i).squeeze().astype(int)\n",
    "\n",
    "    h3_batch.append(cell)\n",
    "    xs_batch.append(xs_one_hot)\n",
    "\n",
    "    if len(h3_batch) >= inference_batch_size:\n",
    "        xs_one_hot_tensor = torch.tensor(np.stack(xs_batch, axis=0), dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            zs_batch = model(xs_one_hot_tensor).cpu().numpy().squeeze().tolist()\n",
    "\n",
    "        coords = [h3.h3_to_geo(h3_index) for h3_index in h3_batch]\n",
    "        lats, lngs = zip(*coords)\n",
    "\n",
    "        _ = qdrant_client.upsert(\n",
    "            collection_name=qdrant_collection,\n",
    "            wait=True,\n",
    "            points=[PointStruct(\n",
    "                id=int(\"0x\" + id, 0),\n",
    "                vector=vector,\n",
    "                payload={\"location\": {\"lon\": lng, \"lat\": lat}}\n",
    "            ) for id, vector, lng, lat in zip(h3_batch, zs_batch, lngs, lats)]\n",
    "        )\n",
    "        h3s_processed = h3s_processed.union(set(h3_batch))\n",
    "        h3_batch = []\n",
    "        xs_batch = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert borders shapefile to geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CPLE_AppDefinedError",
     "evalue": "b'sqlite3_exec(CREATE TRIGGER \"trigger_delete_feature_count_world\" AFTER DELETE ON \"world\" BEGIN UPDATE gpkg_ogr_contents SET feature_count = feature_count - 1 WHERE lower(table_name) = lower(\\'world\\'); END;) failed: disk I/O error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_AppDefinedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/_err.pyx:198\u001b[0m, in \u001b[0;36mfiona._err.GDALErrCtxManager.__exit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_AppDefinedError\u001b[0m: b'sqlite3_exec(CREATE TRIGGER \"trigger_delete_feature_count_world\" AFTER DELETE ON \"world\" BEGIN UPDATE gpkg_ogr_contents SET feature_count = feature_count - 1 WHERE lower(table_name) = lower(\\'world\\'); END;) failed: disk I/O error'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'fiona.ogrext.gdal_flush_cache'\n",
      "Traceback (most recent call last):\n",
      "  File \"fiona/_err.pyx\", line 198, in fiona._err.GDALErrCtxManager.__exit__\n",
      "fiona._err.CPLE_AppDefinedError: b'sqlite3_exec(CREATE TRIGGER \"trigger_delete_feature_count_world\" AFTER DELETE ON \"world\" BEGIN UPDATE gpkg_ogr_contents SET feature_count = feature_count - 1 WHERE lower(table_name) = lower(\\'world\\'); END;) failed: disk I/O error'\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file('lql-data/misc/WB_countries_Admin0_10m/WB_countries_Admin0_10m.shp')\n",
    "new_geoms = [gdf.unary_union]\n",
    "\n",
    "world_gdf = gpd.GeoDataFrame(geometry=new_geoms, crs=gdf.crs)\n",
    "world_gdf.to_file('lql-data/misc/world.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/kygxkvrj2gg1s98_1d7ndy5m0000gn/T/ipykernel_71645/1850854106.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf.geometry = gdf.buffer(0.05).simplify(0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded tmp/world.gpkg to misc/world.gpkg\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "from vectorgeo.transfer import upload_file\n",
    "gdf = gpd.read_file('tmp/world.gpkg')\n",
    "gdf.geometry = gdf.buffer(0.05).simplify(0.1)\n",
    "gdf.to_file('tmp/world.gpkg', driver='GPKG')\n",
    "upload_file('misc/world.gpkg', 'tmp/world.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import os\n",
    "import h3\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rasterio import features\n",
    "import multiprocess\n",
    "\n",
    "from vectorgeo.h3_utils import generate_h3_indexes_at_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate all h3 cells at a given resolution by considering\n",
    "# all hexadecimal numbers of the appropriate length with\n",
    "# f values at the end\n",
    "h3_resolution = 7\n",
    "h3s = generate_h3_indexes_at_resolution(h3_resolution)\n",
    "print(f\"After generating all h3s, there are {len(h3s)} cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading world geometry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/kygxkvrj2gg1s98_1d7ndy5m0000gn/T/ipykernel_87148/3395474982.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  world_gdf.geometry = world_gdf.buffer(0.05).simplify(0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raster\n"
     ]
    }
   ],
   "source": [
    "# Here, we simplify + buffer the geometry and then\n",
    "# convert it into a binary raster mask\n",
    "raster_path = 'tmp/world_mask.tif'\n",
    "x_res = 1000\n",
    "y_res = 1000\n",
    "crs = 'EPSG:4326'\n",
    "\n",
    "print(\"Reading world geometry\")\n",
    "world_gdf = gpd.read_file('tmp/world.gpkg')\n",
    "world_gdf.geometry = world_gdf.buffer(0.05).simplify(0.1)\n",
    "world_gdf = world_gdf.to_crs(crs)\n",
    "\n",
    "# Create the raster\n",
    "print(\"Creating raster\")\n",
    "world_gdf['value'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds should run over all of earth\n",
    "bounds = (-180, -90, 180, 90)\n",
    "\n",
    "transform = rio.transform.from_bounds(*bounds, x_res, y_res)\n",
    "\n",
    "shapes = ((geom, value) for geom, value in zip(world_gdf.geometry, world_gdf.value))\n",
    "image = features.rasterize(\n",
    "            shapes,\n",
    "            out_shape=(y_res, x_res),\n",
    "            transform=transform\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing H3 cells\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1764736/1764736 [00:17<00:00, 101768.28it/s]\n",
      "100%|██████████| 1764736/1764736 [00:19<00:00, 90189.97it/s]\n",
      "100%|██████████| 1764735/1764735 [00:20<00:00, 85450.16it/s]\n",
      "100%|██████████| 1764735/1764735 [00:22<00:00, 79916.03it/s]\n",
      "100%|██████████| 1764735/1764735 [00:22<00:00, 78196.38it/s] \n",
      "100%|██████████| 1764735/1764735 [00:21<00:00, 80254.95it/s] \n",
      "100%|██████████| 1764735/1764735 [00:20<00:00, 84511.17it/s] \n",
      "100%|██████████| 1764735/1764735 [00:19<00:00, 92844.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 0.26936356317470284 of cells\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For all H3 cells, check whether their centroid lands on a 1 cell;\n",
    "# if they do, add them to the set. Otherwise, discard them.\n",
    "n_batches = 8\n",
    "print(\"Processing H3 cells\")\n",
    "h3_batches = [x.tolist() for x in np.array_split(list(h3s), n_batches)]\n",
    "\n",
    "def process_h3_batch(h3s):\n",
    "    h3s_processed = set()\n",
    "\n",
    "    for i, cell in enumerate(tqdm(h3s)):           \n",
    "\n",
    "        centroid = h3.h3_to_geo(cell)\n",
    "        lat, lng = centroid\n",
    "        row, col = rio.transform.rowcol(transform, lng, lat)\n",
    "        val = image[row, col]\n",
    "\n",
    "        if val == 1:\n",
    "            h3s_processed.add(cell)\n",
    "\n",
    "    return h3s_processed\n",
    "\n",
    "with multiprocess.Pool(n_batches) as pool:\n",
    "    h3s_processed = set.union(*pool.map(process_h3_batch, h3_batches))\n",
    "\n",
    "# print fraction of cells which are added to set\n",
    "print(f\"Retained {len(h3s_processed) / len(h3s)} of cells\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/homebrew/lib/python3.11/site-packages (2.9.7)\n",
      "Requirement already satisfied: sqlalchemy in /opt/homebrew/lib/python3.11/site-packages (2.0.20)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.11/site-packages (from sqlalchemy) (4.5.0)\n",
      "\u001b[33mWarning:\u001b[0m postgresql@15 15.4 is already installed and up-to-date.\n",
      "To reinstall 15.4, run:\n",
      "  brew reinstall postgresql@15\n",
      "Service `postgresql@15` already started, use `brew services restart postgresql@15` to restart.\n",
      "sudo: unknown user: postgres\n",
      "sudo: error initializing audit plugin sudoers_audit\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"postgres\" does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39m sudo -u postgres psql -c \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCREATE ROLE postgres LOGIN SUPERUSER;\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Connect to the default 'postgres' database to create a new database\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m conn \u001b[39m=\u001b[39m psycopg2\u001b[39m.\u001b[39;49mconnect(dbname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpostgres\u001b[39;49m\u001b[39m\"\u001b[39;49m, user\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpostgres\u001b[39;49m\u001b[39m\"\u001b[39;49m, host\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlocalhost\u001b[39;49m\u001b[39m\"\u001b[39;49m, port\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m5432\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m conn\u001b[39m.\u001b[39mautocommit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"postgres\" does not exist\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install necessary libraries\n",
    "! pip3 install psycopg2-binary sqlalchemy\n",
    "! brew install postgresql@15\n",
    "\n",
    "# Cell 2: Create a Postgres database locally\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Cell 2: Check if PostgreSQL is running and start it if not\n",
    "import subprocess\n",
    "\n",
    "def is_postgres_running():\n",
    "    try:\n",
    "        result = subprocess.run(['pg_isready'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        return result.returncode == 0\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "if not is_postgres_running():\n",
    "    # This command might vary based on your OS and installation method\n",
    "    # For example, if you installed PostgreSQL via Homebrew on macOS, you'd use `brew services start postgresql`\n",
    "    subprocess.run(['brew', 'services', 'start', 'postgresql@15',])\n",
    "\n",
    "! sudo -u postgres psql -c \"CREATE ROLE postgres LOGIN SUPERUSER;\"\n",
    "\n",
    "# Connect to the default 'postgres' database to create a new database\n",
    "conn = psycopg2.connect(dbname=\"postgres\", user=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"CREATE DATABASE mydatabase;\")\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Cell 3: Install postgis and pgvector extensions\n",
    "engine = create_engine('postgresql://postgres:your_password@localhost:5432/mydatabase')\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(\"CREATE EXTENSION postgis;\")\n",
    "    connection.execute(\"CREATE EXTENSION vector;\")\n",
    "\n",
    "# Cell 4: Create a table and insert test data with 2d spatial coordinates and 3d vector coordinates\n",
    "with engine.connect() as connection:\n",
    "    # Create a table with spatial and vector columns\n",
    "    connection.execute(\"\"\"\n",
    "    CREATE TABLE test_data (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name VARCHAR(255),\n",
    "        location GEOMETRY(Point, 4326),\n",
    "        embedding vector(3)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert test data\n",
    "    connection.execute(\"\"\"\n",
    "    INSERT INTO test_data (name, location, embedding) VALUES\n",
    "    ('Point1', ST_GeomFromText('POINT(1 1)', 4326), '[1,2,3]'),\n",
    "    ('Point2', ST_GeomFromText('POINT(2 2)', 4326), '[4,5,6]'),\n",
    "    ('Point3', ST_GeomFromText('POINT(3 3)', 4326), '[7,8,9]');\n",
    "    \"\"\")\n",
    "\n",
    "# Cell 5: Perform a hybrid vector search bounded by a spatial bounding box\n",
    "search_vector = '[3,1,2]'\n",
    "bounding_box = \"ST_MakeEnvelope(0, 0, 3, 3, 4326)\"  # Define a bounding box from (0,0) to (3,3)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT name, location, embedding\n",
    "FROM test_data\n",
    "WHERE location && {bounding_box} AND embedding <-> ARRAY{search_vector} < 5\n",
    "ORDER BY embedding <-> ARRAY{search_vector} LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "results = engine.execute(query).fetchall()\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
