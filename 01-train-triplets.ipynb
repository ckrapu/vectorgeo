{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 10:01:18.892138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/chriskrapu/Dropbox/projects/vectorgeo/vg-venv/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/chriskrapu/Dropbox/projects/vectorgeo/vg-venv/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/chriskrapu/Dropbox/projects/vectorgeo/vg-venv/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/chriskrapu/Dropbox/projects/vectorgeo/vg-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/chriskrapu/Dropbox/projects/vectorgeo/vg-venv/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import constants as c\n",
    "import yaml\n",
    "import numpy as np\n",
    "import datetime\n",
    "import s3fs\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from models import ResnetTripletEmbedding, triplet_loss\n",
    "from umap import UMAP\n",
    "from tqdm import tqdm\n",
    "\n",
    "from landcover import unpack_array\n",
    "\n",
    "secrets = yaml.load(open(os.path.join(c.BASE_DIR, '.secrets.yml')), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs         = 50\n",
    "batch_size     = 32\n",
    "embed_dim      = 16\n",
    "num_filters    = 64\n",
    "n_linear       = 64\n",
    "n_conv_blocks  = 2\n",
    "n_train_files  = 30\n",
    "model_filename = \"resnet-triplet-lc.keras\"\n",
    "s3_region      = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 files\n",
      "Preparing to read 30 files\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-102420.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-103028.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-108132.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-110513.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-112374.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-11850.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-121111.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-128142.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-131464.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-141004.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-141960.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-142622.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-144478.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-145292.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-145862.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-146462.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-149989.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-151447.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-151487.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-154330.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-155050.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-156334.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-165692.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-168416.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-169779.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-173581.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-175212.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-176102.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-176572.npy\n",
      "....Reading lql-data/landcover/lulc-patches-pairs-32x32-180245.npy\n"
     ]
    }
   ],
   "source": [
    "# Initialize s3fs using aws_aceess_key_id and aws_secret_access_key\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=secrets['aws_access_key_id'],\n",
    "    secret=secrets['aws_secret_access_key'],\n",
    "    client_kwargs={'region_name': s3_region}\n",
    ")\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=secrets['aws_access_key_id'],\n",
    "    aws_secret_access_key=secrets['aws_secret_access_key'],\n",
    "    region_name=s3_region\n",
    ")\n",
    "\n",
    "\n",
    "# Read all files in the bucket c.S3_BUCKET and key 'landcover' with file extension .npy\n",
    "# and store them in a list\n",
    "files = fs.ls(os.path.join(c.S3_BUCKET, 'landcover'))\n",
    "files = [f for f in files if f.endswith('.npy')]\n",
    "print('Found {} files'.format(len(files)))\n",
    "\n",
    "arrays = []\n",
    "\n",
    "files_to_read = files[0:n_train_files]\n",
    "print(f\"Preparing to read {len(files_to_read)} files\")\n",
    "\n",
    "for f in files_to_read:\n",
    "    # Read each file in the list and append it to the arrays list\n",
    "    print('....Reading {}'.format(f))\n",
    "    arrays.append(np.load(fs.open(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from integer to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:17<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "xs_one_hot = np.concatenate([unpack_array(xs) for xs in tqdm(arrays)], axis=0)\n",
    "anchors, positives, negatives = xs_one_hot[:, 0], xs_one_hot[:, 1], xs_one_hot[:, 2]\n",
    "labels = np.zeros((len(anchors), 1))\n",
    "print(f\"Loaded {len(arrays)} files; resulting stacked array has shape {xs_one_hot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xs_one_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Input shape for data is (H, W, C)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m input_shape \u001b[39m=\u001b[39m xs_one_hot\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[1;32m      4\u001b[0m triplet_model, embedding_network \u001b[39m=\u001b[39m initalize_triplet(\n\u001b[1;32m      5\u001b[0m     input_shape,\n\u001b[1;32m      6\u001b[0m     n_conv_blocks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     n_linear\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xs_one_hot' is not defined"
     ]
    }
   ],
   "source": [
    "# Input shape for data is (H, W, C)\n",
    "input_shape = xs_one_hot.shape[2:]\n",
    "\n",
    "triplet_model, embedding_network = initalize_triplet(\n",
    "    input_shape,\n",
    "    n_conv_blocks,\n",
    "    embed_dim,\n",
    "    num_filters,\n",
    "    n_linear\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1282/1282 [==============================] - 336s 255ms/step - loss: 16.8166\n",
      "Epoch 2/5\n",
      "1282/1282 [==============================] - 331s 258ms/step - loss: 6.3468\n",
      "Epoch 3/5\n",
      "1282/1282 [==============================] - 303s 237ms/step - loss: 5.8418\n",
      "Epoch 4/5\n",
      "1282/1282 [==============================] - 326s 254ms/step - loss: 5.4745\n",
      "Epoch 5/5\n",
      "1282/1282 [==============================] - 298s 232ms/step - loss: 5.2063\n"
     ]
    }
   ],
   "source": [
    "history = triplet_model.fit([anchors, positives, negatives], labels, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_triplet_embedding\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 23)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 32, 32, 32)           6656      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)           9248      ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 32, 32, 32)           128       ['conv2d_1[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)           9248      ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 32)           128       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 32, 32, 32)           0         ['conv2d[0][0]',              \n",
      "                                                                     'batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 32)           0         ['add[0][0]']                 \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 32)           9248      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 32)           9248      ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 16, 16, 32)           128       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 32)           9248      ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 16, 16, 32)           128       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 16, 16, 32)           0         ['conv2d_3[0][0]',            \n",
      "                                                                     'batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 32)             0         ['add_1[0][0]']               \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 32)             9248      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 32)             9248      ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 8, 8, 32)             128       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 32)             9248      ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 8, 8, 32)             128       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 8, 8, 32)             0         ['conv2d_6[0][0]',            \n",
      "                                                                     'batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 32)             0         ['add_2[0][0]']               \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 512)                  0         ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 32)                   16416     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   1056      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 32)                   128       ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 32)                   1056      ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 32)                   128       ['dense_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 32)                   0         ['dense[0][0]',               \n",
      "                                                                     'batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 32)                   0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 32)                   1056      ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 32)                   128       ['dense_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 32)                   1056      ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 32)                   128       ['dense_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 32)                   0         ['activation[0][0]',          \n",
      "                                                                     'batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 32)                   0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 8)                    264       ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 102824 (401.66 KB)\n",
      "Trainable params: 102184 (399.16 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting inactive dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components needed to reach 0.99 variance explained: 5\n"
     ]
    }
   ],
   "source": [
    "zs = embedding_network(xs_one_hot[0:1024, 0]).numpy()\n",
    "\n",
    "# transform with pca\n",
    "pca = PCA(n_components=embed_dim)\n",
    "zs_pca = pca.fit_transform(zs)\n",
    "\n",
    "# Print out the number of eigenvalues\n",
    "# needed to reach threshold of variance explained\n",
    "variance_threshold = 0.99\n",
    "variance_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.where(variance_explained > variance_threshold)[0][0]\n",
    "print(f\"Number of components needed to reach {variance_threshold} variance explained: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('temp', model_filename)\n",
    "\n",
    "embedding_network.save(model_path) \n",
    "s3_client.upload_file(model_path, c.S3_BUCKET, f\"models/{model_filename}\")\n",
    "os.remove(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embedding space in 2D / sniff check on embedding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39membed_dim)\n\u001b[0;32m----> 2\u001b[0m zs \u001b[39m=\u001b[39m embedding_network(xs_one_hot[\u001b[39m0\u001b[39m:\u001b[39m1024\u001b[39m, \u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m zs_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(zs)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Create a figure\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_network' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=embed_dim)\n",
    "zs = embedding_network(xs_one_hot[0:1024, 0]).numpy()\n",
    "zs_pca = pca.fit_transform(zs)\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a GridSpec layout\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1])\n",
    "\n",
    "# Create the subplots\n",
    "ax0 = plt.subplot(gs[0, 0])\n",
    "ax1 = plt.subplot(gs[0, 1])\n",
    "ax2 = plt.subplot(gs[1, :])\n",
    "\n",
    "# Plot of embeddings without PCA\n",
    "ax0.imshow(zs[0:64])\n",
    "ax0.set_ylabel(\"Different vectors\")\n",
    "ax0.set_xlabel(\"Vector dimensions\")\n",
    "\n",
    "# Plot of embeddings with PCA\n",
    "ax1.imshow(zs_pca[0:64])\n",
    "ax1.set_ylabel(\"Different vectors\")\n",
    "ax1.set_xlabel(\"Vector dimensions (PCA)\")\n",
    "\n",
    "# Plot of UMAP projection of embeddings\n",
    "reducer = UMAP()\n",
    "ws = reducer.fit_transform(zs)\n",
    "hb = ax2.hexbin(ws[:, 0], ws[:, 1], cmap='viridis', bins='log')\n",
    "ax2.set_title(\"Hexbin log density for\\nUMAP projection of embeddings\")\n",
    "plt.colorbar(hb, ax=ax2, orientation='vertical')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"figures/embeddings.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vg-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
